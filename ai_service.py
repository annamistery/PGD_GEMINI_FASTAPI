import os
from typing import Dict, Any, Optional
import time
import re

# –í—ã–±–µ—Ä–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: "perplexity", "openai", "gemini", "groq"
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "gemini")

if LLM_PROVIDER == "gemini":
    import google.generativeai as genai
elif LLM_PROVIDER == "groq":
    from groq import Groq
elif LLM_PROVIDER in ["openai", "perplexity"]:
    from openai import OpenAI


BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SYSTEM_PROMPT_PATH = os.path.join(BASE_DIR, "system_prompt.txt")


def load_system_prompt() -> str:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–∑ —Ñ–∞–π–ª–∞ –∏–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π."""
    try:
        with open(SYSTEM_PROMPT_PATH, "r", encoding="utf-8") as f:
            prompt = f.read()
            print(f"‚úÖ system_prompt.txt –∑–∞–≥—Ä—É–∂–µ–Ω ({len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return prompt
    except FileNotFoundError:
        print("‚ö†Ô∏è system_prompt.txt –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç")
        return """–¢—ã ‚Äî —Å—Ç–∞—Ä—à–∏–π –ø—Å–∏—Ö–æ–ª–æ–≥-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç, —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø—Ä–æ—Ñ–æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ—É—á.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ (–ú–∞—Ç—Ä–∏—Ü—ã) –∏ —Å–æ—Å—Ç–∞–≤–∏—Ç—å –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ –≥–ª—É–±–æ–∫–∏–π, –≤–¥–æ—Ö–Ω–æ–≤–ª—è—é—â–∏–π –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω–∏–º—ã–π –ø—É—Ç–µ–≤–æ–¥–∏—Ç–µ–ª—å –ø–æ –∂–∏–∑–Ω–∏.

–í–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï:
–¢–µ–±–µ –±—É–¥–µ—Ç –ø–æ–¥–∞–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–æ—á–µ–∫ –∏ –ø–µ—Ä–∏–æ–¥–æ–≤ –∏–∑ PGD-–º–∞—Ç—Ä–∏—Ü—ã.

–ì–õ–ê–í–ù–û–ï –¢–†–ï–ë–û–í–ê–ù–ò–ï (–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û):
1. –í –∏—Ç–æ–≥–æ–≤–æ–º —Ç–µ–∫—Å—Ç–µ –ó–ê–ü–†–ï–©–ï–ù–û –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ü–∏—Ñ—Ä—ã –∞—Ä–∫–∞–Ω–æ–≤, –Ω–æ–º–µ—Ä–∞ —Ç–æ—á–µ–∫ (–¢–æ—á–∫–∞ –ê, –ë, –ì –∏ —Ç.–¥.) –∏–ª–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã —Ä–∞—Å—á—ë—Ç–∞.
2. –ö–ª–∏–µ–Ω—Ç –Ω–µ –¥–æ–ª–∂–µ–Ω –≤–∏–¥–µ—Ç—å –∫—É—Ö–Ω—é —Ä–∞—Å—á–µ—Ç–æ–≤. –û–Ω –¥–æ–ª–∂–µ–Ω –≤–∏–¥–µ—Ç—å —Å–µ–±—è –∏ —Å–≤–æ—é —Å—É–¥—å–±—É.
3. –û–±—Ä–∞—â–∞–π—Å—è –∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –ø–æ –∏–º–µ–Ω–∏ (–Ω–∞–π–¥–∏ –µ–≥–æ –≤ –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞).
4. –°—Ç–∏–ª—å: –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π, —Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–µ—Å–∫–∏–π, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º —á–µ—Ç–∫–∏–π –∏ –¥–µ–ª–æ–≤–æ–π –≤ –≤–æ–ø—Ä–æ—Å–∞—Ö –∫–∞—Ä—å–µ—Ä—ã.

–ü–†–ê–í–ò–õ–ê –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–Ø (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û):
- –ü–∏—à–∏ –¢–û–õ–¨–ö–û —á–∏—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º —Å –∞–±–∑–∞—Ü–∞–º–∏
- –ó–ê–ü–†–ï–©–ï–ù–û –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å markdown-—Å–∏–º–≤–æ–ª—ã: # ## ### ** __ * _ ~~ ``` `
- –ó–ê–ü–†–ï–©–ï–ù–û –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ø–∏—Å–∫–∏ —Å –¥–µ—Ñ–∏—Å–∞–º–∏ –∏–ª–∏ –Ω–æ–º–µ—Ä–∞–º–∏
- –ó–ê–ü–†–ï–©–ï–ù–û –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª—ç—à –∏ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
- –†–∞–∑–¥–µ–ª—è–π –±–ª–æ–∫–∏ –¥–≤—É–º—è –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫–∏ (–ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –º–µ–∂–¥—É –∞–±–∑–∞—Ü–∞–º–∏)
- –í—ã–¥–µ–ª–µ–Ω–∏–µ –¥–µ–ª–∞–π —á–µ—Ä–µ–∑ –ó–ê–ì–õ–ê–í–ù–´–ï —Å–ª–æ–≤–∞ –≤ –Ω–∞—á–∞–ª–µ —Ñ—Ä–∞–∑—ã

–°–¢–†–£–ö–¢–£–†–ê –û–¢–ß–ï–¢–ê (6 –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤):
–ë–õ–û–ö 1. –í—Å—Ç—É–ø–ª–µ–Ω–∏–µ (1 –∞–±–∑–∞—Ü)
–ë–õ–û–ö 2. –¢–≤–æ–π —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç (4-5 –∞–±–∑–∞—Ü–µ–≤)
–ë–õ–û–ö 3. –í–µ–∫—Ç–æ—Ä—ã —Ä–æ—Å—Ç–∞ (4-5 –∞–±–∑–∞—Ü–µ–≤)
–ë–õ–û–ö 4. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∫–∞—Ä—å–µ—Ä–∞ (4-5 –∞–±–∑–∞—Ü–µ–≤)
–ë–õ–û–ö 5. –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∂–∏–∑–Ω–∏ –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º (4-5 –∞–±–∑–∞—Ü–µ–≤)
–ë–õ–û–ö 6. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—á–∞—Å—Ç—å—é (4-5 –∞–±–∑–∞—Ü–µ–≤)
–ó–ê–í–ï–†–®–ï–ù–ò–ï (1 –∞–±–∑–∞—Ü)

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –¢–æ–Ω: —Ç—ë–ø–ª—ã–π, —É–≤–∞–∂–∏—Ç–µ–ª—å–Ω—ã–π, –±–µ–∑ –∏–∑–ª–∏—à–Ω–µ–π —ç–∑–æ—Ç–µ—Ä–∏–∫–∏
- –î–ª–∏–Ω–∞: 3000-3500 —Å–ª–æ–≤
- –ê–±–∑–∞—Ü—ã: 5-7 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∫–∞–∂–¥—ã–π
- –ë–ï–ó —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ markdown-—Ä–∞–∑–º–µ—Ç–∫–∏"""


class LLMConfig:
    def __init__(
        self,
        provider: str,
        model_name: Optional[str] = None,
        temperature: Optional[float] = None,
        max_completion_tokens: Optional[int] = None,
    ):
        self.provider = provider

        if provider == "openai":
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ gpt-4o –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            self.model_name = model_name or os.getenv("OPENAI_MODEL", "gpt-4o")

            # gpt-4o –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É, o1/o3 ‚Äî –Ω–µ—Ç
            if any(m in self.model_name for m in ["o1", "o3"]):
                self.supports_temperature = False
                self.temperature = None
            else:
                self.supports_temperature = True
                self.temperature = (
                    temperature if temperature is not None
                    else float(os.getenv("OPENAI_TEMPERATURE", "0.7"))
                )

            self.max_completion_tokens = (
                max_completion_tokens
                if max_completion_tokens is not None
                else int(os.getenv("OPENAI_MAX_COMPLETION_TOKENS", "4000"))
            )

        elif provider == "perplexity":
            self.model_name = model_name or os.getenv(
                "PERPLEXITY_MODEL", "sonar-pro")
            self.supports_temperature = True
            self.temperature = temperature if temperature is not None else 0.7
            self.max_completion_tokens = max_completion_tokens or 8000

        elif provider == "gemini":
            self.model_name = model_name or os.getenv(
                "GEMINI_MODEL", "gemini-2.5-pro")
            self.supports_temperature = True
            self.temperature = temperature if temperature is not None else 0.6
            self.max_completion_tokens = max_completion_tokens or 8000

        elif provider == "groq":
            self.model_name = model_name or os.getenv(
                "GROQ_MODEL", "llama-3.3-70b-versatile")
            self.supports_temperature = True
            self.temperature = temperature if temperature is not None else 0.6
            self.max_completion_tokens = max_completion_tokens or 8000

    def to_token_params(self) -> dict:
        # –ü–æ–∫–∞ –≤–µ–∑–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä max_tokens
        return {"max_tokens": self.max_completion_tokens}

    def maybe_temperature_arg(self) -> dict:
        if getattr(self, "supports_temperature", False) and self.temperature is not None:
            return {"temperature": self.temperature}
        return {}


class ModelProcessor:
    def __init__(self):
        print(
            f"\nüîß ModelProcessor.__init__() –∑–∞–ø—É—â–µ–Ω (–ø—Ä–æ–≤–∞–π–¥–µ—Ä: {LLM_PROVIDER})")
        self.provider = LLM_PROVIDER
        self.client = None
        self.model = None
        self.system_prompt = load_system_prompt()
        self.config = LLMConfig(provider=self.provider)
        self.model_name = self.config.model_name

        if self.provider == "gemini":
            self._init_gemini()
        elif self.provider == "groq":
            self._init_groq()
        elif self.provider == "openai":
            self._init_openai()
        elif self.provider == "perplexity":
            self._init_perplexity()

    # ---------- INIT ----------

    def _init_openai(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π gpt-4o."""
        from openai import OpenAI  # –∏–º–ø–æ—Ä—Ç –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å gemini-only –æ–∫—Ä—É–∂–µ–Ω–∏–µ

        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("‚ùå OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏")
            return

        try:
            self.client = OpenAI(api_key=api_key)
            # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            test_response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": "ping"}],
                max_tokens=5,
                **self.config.maybe_temperature_arg()
            )
            content = test_response.choices.message.content
            print(f"‚úÖ OpenAI —Ç–µ—Å—Ç —É—Å–ø–µ—à–µ–Ω: {content}")
            print(f"‚úÖ OpenAI {self.model_name} –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ‚úÖ")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ OpenAI: {e}")
            self.client = None

    def _init_perplexity(self):
        from openai import OpenAI  # —Ç–æ—Ç –∂–µ –∫–ª–∏–µ–Ω—Ç, –¥—Ä—É–≥–æ–π base_url

        api_key = os.getenv("PERPLEXITY_API_KEY")
        if not api_key:
            print("‚ùå PERPLEXITY_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏")
            return
        try:
            self.client = OpenAI(
                api_key=api_key, base_url="https://api.perplexity.ai")
            print(f"‚úÖ Perplexity {self.model_name} –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ Perplexity: {e}")

    def _init_gemini(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º GOOGLE_API_KEY (–∏–ª–∏ GEMINI_API_KEY –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π)
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        if not api_key:
            print("‚ùå GOOGLE_API_KEY / GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏")
            return
        try:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel(model_name=self.model_name)
            self.client = True  # –ø—Ä–æ—Å—Ç–æ —Ñ–ª–∞–≥, —á—Ç–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞
            print(f"‚úÖ Gemini {self.model_name} –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ Gemini: {e}")
            self.client = None

    def _init_groq(self):
        from groq import Groq

        api_key = os.getenv("GROQ_API_KEY")
        if not api_key:
            print("‚ùå GROQ_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏")
            return
        try:
            self.client = Groq(api_key=api_key)
            print(f"‚úÖ Groq {self.model_name} –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ Groq: {e}")

    # ---------- CALLS ----------

    def _call_openai(self, pgd_text: str) -> str:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ OpenAI –¥–ª—è gpt-4o."""
        print(f"üîÑ –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ OpenAI API ({self.model_name})...")
        attempts = 3
        for i in range(attempts):
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": pgd_text},
                    ],
                    **self.config.maybe_temperature_arg(),
                    **self.config.to_token_params(),
                )
                return response.choices.message.content.strip()
            except Exception as e:
                print(f"‚ö†Ô∏è –ü–æ–ø—ã—Ç–∫–∞ {i + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if i == attempts - 1:
                    raise
                time.sleep(2)

    def _call_perplexity(self, pgd_text: str) -> str:
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": pgd_text},
            ],
            **self.config.maybe_temperature_arg(),
            **self.config.to_token_params(),
        )
        return response.choices.message.content.strip()

    def _call_gemini(self, pgd_text: str) -> str:
        full_prompt = f"{self.system_prompt}\n\n{pgd_text}"
        response = self.model.generate_content(
            full_prompt,
            generation_config={
                "temperature": self.config.temperature,
                "max_output_tokens": self.config.max_completion_tokens,
            },
        )
        return response.text.strip()

    def _call_groq(self, pgd_text: str) -> str:
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": pgd_text},
            ],
            **self.config.maybe_temperature_arg(),
            **self.config.to_token_params(),
        )
        return response.choices.message.content.strip()

    # ---------- PUBLIC API ----------

    def get_llm_response(self, pgd_data: Dict[str, Any], user_info: Dict[str, str]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –æ—Ç—á—ë—Ç."""
        print("\n" + "=" * 70)
        print(f"üöÄ get_llm_response() –≤—ã–∑–≤–∞–Ω ({self.provider})")

        if not self.client:
            return self._fallback_response(pgd_data, user_info)

        try:
            name = user_info.get("name", "–ö–ª–∏–µ–Ω—Ç")
            lines = [f"–ò–º—è: {name}"]
            if user_info.get("dob"):
                lines.append(f"–î–∞—Ç–∞ —Ä–æ–∂–¥–µ–Ω–∏—è: {user_info['dob']}")
            if user_info.get("gender"):
                lines.append(f"–ü–æ–ª: {user_info['gender']}")
            lines.append("\nPGD-–ú–ê–¢–†–ò–¶–ê:")

            for block_name, block_value in pgd_data.items():
                lines.append(f"\nüìå {block_name.upper()}:")
                if isinstance(block_value, dict):
                    for k, v in block_value.items():
                        lines.append(f"  ‚Ä¢ {k}: {v}")
                else:
                    lines.append(f"  {block_value}")

            pgd_text = "\n".join(lines)

            call_methods = {
                "openai": self._call_openai,
                "perplexity": self._call_perplexity,
                "gemini": self._call_gemini,
                "groq": self._call_groq,
            }

            response_text = call_methods[self.provider](pgd_text)
            return self._clean_markdown(response_text)

        except Exception as e:
            print(f"‚ùå –û–®–ò–ë–ö–ê get_llm_response: {e}")
            return self._fallback_response(pgd_data, user_info)

    # ---------- CHAT METHOD ----------

    def chat_with_report(self, report_text: str, question: str) -> str:
        """–ß–∞—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–æ—Ç–æ–≤–æ–≥–æ –æ—Ç—á—ë—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞."""
        if not self.client:
            return "–û—à–∏–±–∫–∞: –ú–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞."

        print(f"\nüí¨ CHAT –≤—ã–∑–≤–∞–Ω –¥–ª—è {self.model_name}")
        print(
            f"üìè –û—Ç—á–µ—Ç: {len(report_text)} | –í–æ–ø—Ä–æ—Å: {len(question)} —Å–∏–º–≤–æ–ª–æ–≤")

        chat_system_prompt = (
            "–¢—ã ‚Äî —Ç–æ—Ç –∂–µ –ø—Å–∏—Ö–æ–ª–æ–≥-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—Å—Ç–∞–≤–∏–ª —ç—Ç–æ—Ç –æ—Ç—á–µ—Ç. "
            "–û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∫–ª–∏–µ–Ω—Ç–∞ –ö–†–ê–¢–ö–û (1-3 –∞–±–∑–∞—Ü–∞), —Ç–µ–ø–ª–æ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ. "
            "–û—Å–Ω–æ–≤—ã–≤–∞–π —Å–≤–æ–∏ —Å–æ–≤–µ—Ç—ã —Ç–æ–ª—å–∫–æ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞. "
            "–°–æ–±–ª—é–¥–∞–π —Å—Ç—Ä–æ–≥–∏–π –∑–∞–ø—Ä–µ—Ç –Ω–∞ markdown-—Ä–∞–∑–º–µ—Ç–∫—É (–Ω–∏–∫–∞–∫–∏—Ö **, #, —Å–ø–∏—Å–∫–æ–≤)."
        )

        # –ö–æ–Ω—Ç–µ–∫—Å—Ç (–æ–±—Ä–µ–∑–∞–µ–º –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ª–∏–º–∏—Ç–æ–≤)
        context = report_text[:15000]

        try:
            if self.provider in ["openai", "perplexity", "groq"]:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": chat_system_prompt},
                        {
                            "role": "assistant",
                            "content": f"–í–æ—Ç —Ç–≤–æ–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –æ—Ç—á–µ—Ç: {context}",
                        },
                        {"role": "user", "content": question},
                    ],
                    **self.config.maybe_temperature_arg(),
                    max_tokens=1000,
                )
                answer = response.choices.message.content.strip()

            elif self.provider == "gemini":
                full_chat_prompt = (
                    f"{chat_system_prompt}\n\n"
                    f"–ö–û–ù–¢–ï–ö–°–¢ –û–¢–ß–ï–¢–ê:\n{context}\n\n"
                    f"–í–û–ü–†–û–° –ö–õ–ò–ï–ù–¢–ê: {question}"
                )
                response = self.model.generate_content(full_chat_prompt)
                answer = response.text.strip()

            else:
                return "–ú–µ—Ç–æ–¥ —á–∞—Ç–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞."

            return self._clean_markdown(answer)

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —á–∞—Ç–µ: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–æ–∑–Ω–∏–∫–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–Ω–∞—á–µ."

    # ---------- UTILS ----------

    def _clean_markdown(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –æ—Ç markdown –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é –ø—Ä–æ–º–ø—Ç–∞."""
        text = text.replace("\\n", "\n").replace("\\", "")
        text = re.sub(r"^#{1,6}\s+", "", text, flags=re.MULTILINE)
        text = re.sub(r"\*\*(.+?)\*\*", r"\1", text)
        text = re.sub(r"\*(.+?)\*", r"\1", text)
        text = re.sub(r"`(.+?)`", r"\1", text)
        text = re.sub(r"^[ \t]*[-‚Ä¢*]\s+", "", text, flags=re.MULTILINE)
        text = re.sub(r"\n{3,}", "\n\n", text)
        return text.strip()

    def _fallback_response(self, pgd_data: Dict[str, Any], user_info: Dict[str, str]) -> str:
        name = user_info.get("name", "–ö–ª–∏–µ–Ω—Ç")
        return f"–ü–æ—Ä—Ç—Ä–µ—Ç –¥–ª—è {name} –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (LLM –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç)."
